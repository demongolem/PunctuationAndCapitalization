# -*- coding: utf-8 -*-
"""PAC-Keras

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11giMGybz97skSxyNOrPTQ8rd506ZXxSm
"""

# -*- coding: utf-8 -*-
# imports
import ast
from collections import Counter
from imblearn.over_sampling import SMOTENC
from joblib import dump, load
from keras.utils import to_categorical
from keras.models import Sequential, load_model
from keras.layers import Embedding, LSTM, Dense, Dropout
import numpy as np
import os
import pandas as pd
import random
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.utils import resample
from sklearn.utils.multiclass import unique_labels
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
import string
import tensorflow as tf
from zipfile import ZipFile
from scipy.sparse.linalg.matfuncs import _is_upper_triangular

# test GPU / Support
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

file_name = '../datasets/Newswire/Newswire.zip'

with ZipFile(file_name, 'r') as zipt:
    zipt.extractall()
    print('Done')

ENTERTAINMENT_DIR = 'entertainment/bodies'
INTERNATIONAL_DIR = 'international/bodies'
NATIONAL_DIR = 'national/bodies'
POLITICS_DIR = 'politics/bodies'
SPORTS_DIR = 'sports/bodies'

nlp = English()
tokenizer = Tokenizer(nlp.vocab)
unknown_token = '<unk>'

index2word = None
index2word_y = None
word2index = None
word2index_y = None

def initialize_indices(y_name):
    global index2word, index2word_y, word2index, word2index_y
    index2word = [unknown_token]
    word2index = {unknown_token: 0}
    if y_name == 'is_upper':
        index2word_y = []
        word2index_y = {}
    else:
        index2word_y = [unknown_token]
        word2index_y = {unknown_token: 0}

def fetch_dictionary(filename):
    with open(filename, "r", encoding='utf-8') as file:
        contents = file.read()
    return ast.literal_eval(contents)

# should be called after tokenization but before changing the size of the dataset
def encode_tokens(X_train):
    list1 = X_train['prev_char'].tolist()
    list2 = X_train['curr_char'].tolist()
    list3 = X_train['next_char'].tolist()
    list4 = X_train['id'].tolist()

    combined_list = []
    for l1, l2, l3, _ in zip(list1, list2, list3, list4):
        combined_list.append(l1)
        if l3 == '<END>':
            combined_list.append(l2)
            combined_list.append(l3)

    counter = Counter(combined_list)
    for word, _ in counter.items():
        index2word.append(word)
        word2index[word] = len(word2index)

    dump(index2word, 'models/index2word.joblib')

def train_rf(X_train, y_train, y_column_name, model_name):
    #Create a Gaussian Classifier
    clf=RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)

    X_train['prev_char'] = X_train['prev_char'].transform(lambda x: word2index[x]).copy(deep=False)
    X_train['curr_char'] = X_train['curr_char'].transform(lambda x: word2index[x]).copy(deep=False)
    X_train['next_char'] = X_train['next_char'].transform(lambda x: word2index[x]).copy(deep=False)

    values = y_train[y_column_name].tolist()
    counter = Counter(values)
    for word, _ in counter.items():
        index2word_y.append(word)
        word2index_y[word] = len(word2index_y)

    y_train[y_column_name] = y_train[y_column_name].transform(lambda x: word2index_y[x]).copy(deep=False)

    del X_train['id']
    del y_train['id']

    #Train the model using the training sets y_pred=clf.predict(X_test)
    clf.fit(X_train,y_train.values.ravel())
    
    if model_name:
        dump(clf, model_name)

def train(X_train, y_train, y_column_name, epochs=5, model_name=None):    
    
    X_train['prev_char'] = X_train['prev_char'].transform(lambda x: word2index[x]).copy(deep=False)
    X_train['curr_char'] = X_train['curr_char'].transform(lambda x: word2index[x]).copy(deep=False)
    X_train['next_char'] = X_train['next_char'].transform(lambda x: word2index[x]).copy(deep=False)

    values = y_train[y_column_name].tolist()
    counter = Counter(values)
    for word, _ in counter.items():
        index2word_y.append(word)
        word2index_y[word] = len(word2index_y)

    if y_column_name == 'next_punct':
        dump(index2word_y, 'models/index2word_y_next_punct.joblib')
    elif y_column_name == 'is_upper':
        dump(index2word_y, 'models/index2word_y_is_upper.joblib')
        
    y_train[y_column_name] = y_train[y_column_name].transform(lambda x: word2index_y[x]).copy(deep=False)

    with open('models/word2index.txt', 'w', encoding='utf-8') as f:
        print(word2index, file=f)
    with open('models/word2index_y.txt', 'w', encoding='utf-8') as f:
        print(word2index_y, file=f)

    del X_train['id']
    del y_train['id']

    num_classes = len(word2index)
    maxlen = len(X_train.columns)

    num_output_classes = len(word2index_y)
    
    embedding_size = 50
    hidden_layer_size = int((num_output_classes + maxlen) / 2)
    
    model = Sequential()
    model.add(Embedding(input_dim = num_classes, output_dim = embedding_size, input_length = maxlen))
    model.add(LSTM(hidden_layer_size, return_sequences = True))
    model.add(Dropout(0.2))
    model.add(LSTM(hidden_layer_size))
    model.add(Dropout(0.2))
    model.add(Dense(num_output_classes, activation = 'softmax'))
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')

    print('This is the model summary')
    print(model.summary())
    print('End of model summary')    

    y_train = to_categorical(y_train)

    print(X_train.shape)
    print(y_train.shape)
    X_train.to_csv('dataframes/final_x_train.csv', index=False)        
    np.savetxt('dataframes/final_y_train.csv', y_train,fmt='%s', delimiter=",")
    model.fit(X_train, y_train, epochs=epochs)    
    model.save(model_name)

def predict_rf(X_test, model_name):
    clf = load(model_name)
    X_test['prev_char'] = X_test['prev_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)
    X_test['curr_char'] = X_test['curr_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)
    X_test['next_char'] = X_test['next_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)
    del X_test['id']
    y_pred = clf.predict(X_test)
    return y_pred

def predict(model_name, X_test, word2index= None):
    model = load_model(model_name)
    X_test['prev_char'] = X_test['prev_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)
    X_test['curr_char'] = X_test['curr_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)
    X_test['next_char'] = X_test['next_char'].transform(lambda x: word2index[unknown_token] if x not in word2index else word2index[x]).copy(deep=False)

    y_pred = model.predict(np.asarray(X_test[['prev_char', 'curr_char', 'next_char']]).astype(np.int16), verbose=2)
    return y_pred

def my_train_test_split(X, y, test_size=0.2, random_state = 0):
    random.seed(random_state)
    
    id_set = set(X['id'].tolist())
    test_id_set = set()
    train_id_set = set()
 
    for single_id in id_set:
        if random.random() < test_size:
            test_id_set.add(single_id)
        else:
            train_id_set.add(single_id)

    X_train = X.loc[X['id'].isin(train_id_set)]
    X_test = X.loc[X['id'].isin(test_id_set)]
    y_train = y.loc[y['id'].isin(train_id_set)]
    y_test = y.loc[y['id'].isin(test_id_set)]
    
    return X_train, X_test, y_train, y_test

def which_punct(c):
    if c in string.punctuation:
        return str(c)
    return "NONE"

def tokenize_column(df, text_column):
    return df[text_column].apply(lambda x: tokenizer(x))

def create_df_from_text_lists(texts, texts_bare, ids):
    rows = []
    for t,bt,this_id in zip(texts, texts_bare, ids):
        number = 0
        for bare_number in range(0, len(bt)):
            bc = bt[bare_number]
            while True:
                c = t[number]
                if bc.lower() == c.lower():
                    break
                number += 1
            entry = dict()
            entry['is_start'] = bare_number == 0
            entry['curr_char'] = str(bc)
            if bare_number == 0:
                entry['prev_char'] = "<START>"
            else:
                entry['prev_char'] = str(bt[bare_number - 1])
            if bare_number == len(bt) - 1:
                entry['next_char'] = "<END>"
            else:
                entry['next_char'] = str(bt[bare_number + 1])            
            if bc != c:
                entry['is_upper'] = True
            else:
                entry['is_upper'] = False
            if number != len(t) - 1:
                entry['next_punct'] = which_punct(t[number + 1])
            else:
                entry['next_punct'] = 'NONE'
            entry['id'] = this_id
            
            rows.append(entry)
            number += 1
            
    return pd.DataFrame(rows)

def read_into_pandas():
    text_list = []
    
    for f in os.listdir(ENTERTAINMENT_DIR):
        with open(os.path.join(ENTERTAINMENT_DIR, f), 'r', encoding='utf-8') as fs:
            text_list.append({'id': f[0:-4], 'text': fs.read(), 'category':'ENTERTAINMENT'})

    for f in os.listdir(INTERNATIONAL_DIR):
        with open(os.path.join(INTERNATIONAL_DIR, f), 'r', encoding='utf-8') as fs:
            text_list.append({'id': f[0:-4], 'text': fs.read(), 'category':'INTERNATIONAL'})

    for f in os.listdir(NATIONAL_DIR):
        with open(os.path.join(NATIONAL_DIR, f), 'r', encoding='utf-8') as fs:
            text_list.append({'id': f[0:-4], 'text': fs.read(), 'category':'NATIONAL'})
    
    for f in os.listdir(POLITICS_DIR):
        with open(os.path.join(POLITICS_DIR, f), 'r', encoding='utf-8') as fs:
            text_list.append({'id': f[0:-4], 'text': fs.read(), 'category':'POLITICS'})

    for f in os.listdir(SPORTS_DIR):
        with open(os.path.join(SPORTS_DIR, f), 'r', encoding='utf-8') as fs:
            text_list.append({'id': f[0:-4], 'text': fs.read(), 'category':'SPORTS'})

    return pd.DataFrame(text_list, columns=['id', 'text', 'category'])

def read_in_data():
    # read in data
    df = read_into_pandas()
    return df

def manipulate_data(df, csv_file):
    # manipulate data / data features
    # .replace('\n', ' ').replace('\t', ' ').strip()
    df['text_bare'] = [t.lower().translate(str.maketrans('', '', string.punctuation)) for t in df['text']]
    df['bare_tokens'] = tokenize_column(df, 'text_bare')
    print(df.columns)
    
    all_texts = df['text'].tolist()
    all_bare_texts = df['text_bare'].tolist()
    all_ids = df['id'].to_list()
    
    df_punct = create_df_from_text_lists(all_texts, all_bare_texts, all_ids)
    df_punct.to_csv(csv_file)
    
    return df_punct

def form_X(df_punct):
    X = df_punct[["prev_char", "curr_char", "next_char"]]
    X.to_csv('dataframes/X.csv')
    return X

def form_y(df_punct, y_name):
    y = df_punct[[y_name]]
    return y

def split_data_keras(df_punct, y_name):
    X_train_keras,X_test_keras,y_train_keras,y_test_keras = my_train_test_split(df_punct[["id", "prev_char", "curr_char", "next_char"]], df_punct[["id", y_name]])
    X_train_keras.to_csv('dataframes/X_train_keras.csv')
    y_train_keras.to_csv('dataframes/y_train_keras.csv')
    X_test_keras.to_csv('dataframes/X_test_keras.csv')
    y_test_keras.to_csv('dataframes/y_test_keras.csv')
    return (X_train_keras, y_train_keras, X_test_keras, y_test_keras)

def evaluate_keras(y_name, X_test_keras, y_test_keras):
    word2index = fetch_dictionary('models/word2index.txt')
    word2index_y = fetch_dictionary('models/word2index_y.txt')

    y_pred_keras = predict('models/LSTM_' + y_name + '.joblib', X_test_keras, 
                                     word2index = word2index)

    # nope, there is not one column!
    # we need to reverse this is we are to proceed as before
    y_pred_keras = np.argmax(y_pred_keras, axis=1)
    y_pred_keras = pd.DataFrame(y_pred_keras, columns=[y_name])
    y_test_keras[y_name] = y_test_keras[y_name].transform(lambda x: word2index_y[unknown_token] if x not in word2index_y else word2index_y[x]).copy(deep=False)
    y_test_keras = pd.DataFrame(y_test_keras, columns=[y_name])
    
    y_test_keras.reset_index(drop=True, inplace=True)
    y_comparison = pd.DataFrame(np.where(y_pred_keras == y_test_keras, 1, 0), columns=['correct'])
    y_pred_keras.columns = ['pred']
    y_test_keras.columns = ['truth']
    res_df = pd.concat([y_pred_keras, y_test_keras, y_comparison], axis=1)
    res_df.to_csv('results/Y_LSTM_results_' + y_name + '.csv')
    
    if y_name == 'next_punct':
        print(f1_score(y_test_keras, y_pred_keras, average=None, pos_label=True))
    else:
        print(f1_score(y_test_keras, y_pred_keras, average="binary", pos_label=True))

    conf_df = pd.DataFrame(confusion_matrix(y_test_keras, y_pred_keras))
    orig_text_labels = index2word_y
    labels = unique_labels(y_test_keras['truth'], y_pred_keras['pred'])
    conf_df.columns = [orig_text_labels[l] for l in labels] 
    conf_df.index = conf_df.columns
    conf_df.to_csv('results/Y_LSTM_confusion_matrix_' + y_name +'.txt')

#we might want to consider not just punct/no_punct, but drill down to individual classes
def oversample(y_name,X_train_keras, y_train_keras):
    # concatenate our training data back together
    X = pd.concat([X_train_keras, y_train_keras], axis=1)
    X.columns = ['id', 'prev_char', 'curr_char', 'next_char', 'id2', y_name]
    print('Concat x shape')
    print(X.shape)

    # separate minority and majority classes
    no_punct = X[X[y_name]=='NONE']
    yes_punct = X[X[y_name]!='NONE']
    print('no punct shape')
    print(no_punct.shape)
    print('yes punct shape')
    print(yes_punct.shape)

    # upsample minority
    yes_punct_upsampled = resample(yes_punct,
                    replace=True, # sample with replacement
                    n_samples=len(no_punct), # match number in majority class
                    random_state=0) # reproducible results
    print('yes punct upsampled shape')
    print(yes_punct_upsampled.shape)

    # combine majority and upsampled minority
    upsampled = pd.concat([no_punct, yes_punct_upsampled])
    print('upsampled columns')
    print(upsampled.columns)
    print('upsampled shape')
    print(upsampled.shape)

    y_train_keras = pd.DataFrame(upsampled[y_name], columns=[y_name])
    y_train_keras['id'] = upsampled.id2
    X_train_keras = upsampled.drop(y_name, axis=1)
    X_train_keras = X_train_keras.drop('id2', axis=1)

    print('Resamp x train')
    print(X_train_keras.columns)
    print('Resamp y train')
    print(y_train_keras.columns)

    print('Resamp x train shape')
    print(X_train_keras.shape)
    print('Resamp y train shape')
    print(y_train_keras.shape)

    return (X_train_keras, y_train_keras)

def undersample(y_name, X_train_keras, y_train_keras):
    # concatenate our training data back together
    X = pd.concat([X_train_keras, y_train_keras], axis=1)
    X.columns = ['id', 'prev_char', 'curr_char', 'next_char', 'id2', y_name]

    # separate minority and majority classes
    if y_name == 'next_punct':
        majority_class = X[X[y_name]=='NONE']
        non_majority_class = X[X[y_name]!='NONE']
    elif y_name == 'is_upper':
        majority_class = X[X[y_name]==False]
        non_majority_class = X[X[y_name]==True]

    no_punct_downsampled = resample(majority_class,
                                replace = False, # sample without replacement
                                n_samples = len(non_majority_class), # match minority n
                                random_state = 0) # reproducible results

    # combine minority and downsampled majority
    downsampled = pd.concat([no_punct_downsampled, non_majority_class])

    y_train_keras = pd.DataFrame(downsampled[y_name], columns=[y_name])
    y_train_keras['id'] = downsampled.id2
    X_train_keras = downsampled.drop(y_name, axis=1)
    X_train_keras = X_train_keras.drop('id2', axis=1)

    return (X_train_keras, y_train_keras)

def smote(y_name, X_train_keras, y_train_keras):
#    sm = SMOTENC(categorical_features=['prev_char', 'curr_char', 'next_char'], random_state=0, sampling_strategy=0.6)
    sm = SMOTENC(categorical_features=[0, 1, 2], random_state=0)
    X_train_keras['spurrious'] = 0.0
    X_train_2, y_train_2 = sm.fit_sample(X_train_keras[['prev_char', 'curr_char', 'next_char', 'spurrious']], y_train_keras[y_name])
    del X_train_2["spurrious"]
    print(X_train_2.head())
    print(y_train_2.head())
    return (X_train_keras, y_train_keras)

# *********************************************************
# here we start with the "main code"
# *********************************************************

def main():
    
    labels = ['next_punct', 'is_upper']
    methods = ['Keras']
    #methods = ['RandomForest']
    epochs = 5
    
    df = read_in_data()
    
    df_punct = manipulate_data(df, 'dataframes/df_punct.csv')
    
    X = form_X(df_punct)
    
    for y_name in labels:
        print('Working with ' + y_name)
        # this must be reset before moving to another classification problem
        initialize_indices(y_name)
        y = form_y(df_punct, y_name)
    
        for method in methods:
    
            if 'RandomForest' == method:
                (X_train, y_train, X_test, y_test) = split_data_keras(df_punct, y_name)
    
                # encode tokens
                # we could do this before the split of X matrix, perhaps even better operation
                encode_tokens(X_train)
    
                # This is not a balanced dataset. Here are some potential solutions
                #(X_train, y_train) =  oversample(y_name, X_train, y_train)
                (X_train, y_train) =  undersample(y_name, X_train, y_train)
                #(X_train, y_train) =  smote(y_name, X_train, y_train)
    
                print('Random Forest train begin')
                train_rf(X_train, y_train, y_name, 'models/RF_' + y_name + '.joblib')
                print('Random Forest train end')
    
                print('Random Forest predict begin')
                y_pred = predict_rf(X_test, 'models/RF_' + y_name + '.joblib')
                y_pred = pd.DataFrame(y_pred, columns=[y_name])
                y_test[y_name] = y_test[y_name].transform(lambda x: word2index_y[unknown_token] if x not in word2index_y else word2index_y[x]).copy(deep=False)
                y_test = pd.DataFrame(y_test, columns=[y_name])          
                y_test.reset_index(drop=True, inplace=True)
                y_comparison = pd.DataFrame(np.where(y_pred == y_test, 1, 0), columns=['correct'])
                y_pred.columns = ['pred']
                y_test.columns = ['truth']
                res_df = pd.concat([y_pred, y_test, y_comparison], axis=1)
                res_df.to_csv('results/Y_results_' + y_name + '.csv')
              
                if y_name == 'next_punct':
                    print(f1_score(y_test, y_pred, average=None, pos_label=True))
                else:
                    print(f1_score(y_test, y_pred, average="binary", pos_label=True))
                cm = confusion_matrix(y_test, y_pred)
                with open('results/Y_rf_confusion_matrix_' + y_name +'.txt', 'w') as fs:
                    fs.write(str(cm))
                print('Random Forest predict end')
            elif 'BERT' == method:
                # this path obviously has not been fully developed
                print('BERT begin')
                Bert.train(X_train_bert, y_train_bert, y_name, 'models/BERT_' + y_name + '.joblib')
                print('BERT LSTM end')
            elif 'Keras' == method:
                (X_train_keras, y_train_keras, X_test_keras, y_test_keras) = split_data_keras(df_punct, y_name)
    
                # encode tokens
                # we could do this before the split of X matrix, perhaps even better operation
                encode_tokens(X_train_keras)          
    
                print('Orig x train')
                print(X_train_keras.columns)
                print('Orig x shape')
                print(X_train_keras.shape)
                print('Orig y shape')
                print(y_train_keras.shape)
    
                # This is not a balanced dataset. Here are some potential solutions
                if y_name == 'next_punct':
                    #(X_train_keras, y_train_keras) =  undersample(y_name, X_train_keras, y_train_keras)
                    pass
                elif y_name == 'is_upper':
                    #(X_train_keras, y_train_keras) =  oversample(y_name, X_train_keras, y_train_keras)
                    #(X_train_keras, y_train_keras) = smote(y_name, X_train_keras, y_train_keras)
                    pass
    
                print('After undersample x shape')
                print(X_train_keras.shape)
                print('After undersample y shape')
                print(y_train_keras.shape)
    
                # put the new dataframes to disc, for sanity check
                X_train_keras.to_csv('dataframes/X_train_keras_after_rebalance.csv', index=False)
                np.savetxt('dataframes/y_train_keras_after_rebalance.csv', y_train_keras,fmt='%s,%s', delimiter=",")
    
                print('Keras LSTM train begin')
                train(X_train_keras, y_train_keras, y_name, epochs=epochs, model_name='models/LSTM_' + y_name + '.joblib')
                print('Keras LSTM train end')
              
                print('Keras LSTM test begin')
                evaluate_keras(y_name, X_test_keras, y_test_keras)
                print('Keras LSTM test end')
            else:
                print('Unknown method ' + method)

def reproduce_test_text():
    X_test_df = pd.read_csv('dataframes/X_test_keras.csv')
    y_next_punct = pd.read_csv('results/Y_LSTM_results_next_punct.csv')
    y_is_upper = pd.read_csv('results/Y_LSTM_results_is_upper.csv')

    index2word = load('models/index2word.joblib')
    index2word_y_is_upper = load('models/index2word_y_is_upper.joblib')
    index2word_y_next_punct = load('models/index2word_y_next_punct.joblib')

    output_document = ''    
    for i in range(0, X_test_df.shape[0]):
        is_upper = index2word_y_is_upper[y_is_upper.iloc[i]['pred']]
        next_punct = index2word_y_next_punct[y_next_punct.iloc[i]['pred']]
        if is_upper:
            output_document += X_test_df.iloc[i]['curr_char'].upper()
        else:
            output_document += X_test_df.iloc[i]['curr_char']
        if next_punct != 'NONE':
            output_document += next_punct
    with open('results/predicted_documents.txt', 'w', encoding='utf-8') as fs:
        fs.write(output_document)
            
if __name__ == "__main__":
    main()
    reproduce_test_text()